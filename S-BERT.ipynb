{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[Reference Code](https://www.pinecone.io/learn/series/nlp/train-sentence-transformers-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set GPU device\n",
    "device = torch.device(\"mps\")\n",
    "device\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       "  'idx': Value(dtype='int32', id=None)},\n",
       " {'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "snli = datasets.load_dataset('snli')\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features, snli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([-1,  0,  1,  2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "# Merge the two DatasetDict objects\n",
    "\n",
    "# todo increase range fot better results\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': datasets.concatenate_datasets([snli['train'], mnli['train']]).shuffle(seed=55).select(list(range(10))),\n",
    "    'test': datasets.concatenate_datasets([snli['test'], mnli['test_mismatched']]).shuffle(seed=55).select(list(range(10))),\n",
    "    'validation': datasets.concatenate_datasets([snli['validation'], mnli['validation_mismatched']]).shuffle(seed=55).select(list(range(10)))\n",
    "})\n",
    "#remove .select(list(range(1000))) in order to use full dataset\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_seq_length = 128\n",
    "    padding = 'max_length'\n",
    "    # Tokenize the premise\n",
    "    premise_result = tokenizer(\n",
    "        examples['premise'], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    #num_rows, max_seq_length\n",
    "    # Tokenize the hypothesis\n",
    "    hypothesis_result = tokenizer(\n",
    "        examples['hypothesis'], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    #num_rows, max_seq_length\n",
    "    # Extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    #num_rows\n",
    "    return {\n",
    "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
    "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
    "        \"labels\" : labels\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise','hypothesis','label'])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)\n",
    "    print(batch['premise_attention_mask'].shape)\n",
    "    print(batch['hypothesis_input_ids'].shape)\n",
    "    print(batch['hypothesis_attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start from a pretrained bert-base-uncased model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "## Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t ∈  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "## Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n",
    "\n",
    "<img src=\"./figures/sbert-architecture.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    \n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/sbert-ablation.png\" width=\"350\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 1.152026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:37<00:00, 157.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 1.140500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:09<00:00, 309.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 1.143302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:05<00:00, 185.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 1.152462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:33<00:00, 153.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 1.141972\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 5\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "        \n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u = model(inputs_ids_a, attention_mask=attention_a)  \n",
    "        v = model(inputs_ids_b, attention_mask=attention_b)  \n",
    "\n",
    "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "         # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "        \n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "        \n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "        \n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x) #batch_size, classifer\n",
    "        \n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "        \n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        scheduler.step() # update learning rate scheduler\n",
    "        scheduler_classifier.step()\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.2409\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "prediction_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # prepare batches and move all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "        \n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "        # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u, attention_a)  # (batch_size, hidden_dim)\n",
    "        v_mean_pool = mean_pool(v, attention_b)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Calculate cosine similarity \n",
    "        batch_similarity = torch.nn.functional.cosine_similarity(u_mean_pool, v_mean_pool, dim=-1)\n",
    "        total_similarity += batch_similarity.sum().item()\n",
    "\n",
    "        # Combine features for the classifier: [u, v, |u - v|]\n",
    "        difference_uv = torch.abs(u_mean_pool - v_mean_pool)  # (batch_size, hidden_dim)\n",
    "        classifier_input = torch.cat([u_mean_pool, v_mean_pool, difference_uv], dim=-1)  # (batch_size, 3*hidden_dim)\n",
    "\n",
    "        # Pass from classifier head\n",
    "        logits = classifier_head(classifier_input) \n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # Determine the predicted class for each sample\n",
    "        predicted_classes = torch.argmax(logits, dim=-1) \n",
    "\n",
    "        # Save predictions\n",
    "        predictions.extend(predicted_classes.detach().cpu().tolist())\n",
    "        true_labels.extend(label.detach().cpu().tolist())\n",
    "        prediction_probabilities.extend(probabilities.detach().cpu().tolist())\n",
    "\n",
    "# average cosine similarity\n",
    "average_similarity = total_similarity / (len(eval_dataloader) * eval_dataloader.batch_size)\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.22      1.00      0.36         2\n",
      "      neutral       0.00      0.00      0.00         5\n",
      "contradiction       1.00      0.33      0.50         3\n",
      "\n",
      "     accuracy                           0.30        10\n",
      "    macro avg       0.41      0.44      0.29        10\n",
      " weighted avg       0.34      0.30      0.22        10\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# classification report\n",
    "class_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "print(classification_report(true_labels, predictions, target_names=[\"entailment\", \"neutral\", \"contradiction\"]))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8057\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs_a = tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    inputs_b = tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = inputs_a['input_ids']\n",
    "    attention_a = inputs_a['attention_mask']\n",
    "    inputs_ids_b = inputs_b['input_ids']\n",
    "    attention_b = inputs_b['attention_mask']\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "huggingface_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Pairs Cosine Similarity :\n",
      "My Model: 0.7128\n",
      "all-MiniLM-L6-v2 Model: 0.4262\n",
      "\n",
      "Neutral Pairs Cosine Similarity :\n",
      "My Model: 0.6519\n",
      "all-MiniLM-L6-v2 Model: 0.0947\n",
      "\n",
      "Contradiction Pairs Cosine Similarity :\n",
      "My Model: 0.8349\n",
      "all-MiniLM-L6-v2 Model: 0.4858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_print_similarity(model, tokenizer, hf_model, sentence_a, sentence_b, device, pair_type):\n",
    "    print(f\"{pair_type} Pairs Cosine Similarity :\")\n",
    "    similarity = torch.tensor(calculate_similarity(model, tokenizer, sentence_a, sentence_b, device))\n",
    "    hf_similarity = calculate_similarity_hf(hf_model, sentence_a, sentence_b)\n",
    "    print(f\"My Model: {similarity:.4f}\")\n",
    "    print(f\"all-MiniLM-L6-v2 Model: {hf_similarity:.4f}\")\n",
    "    print()\n",
    "\n",
    "def calculate_similarity_hf(model, sentence_a, sentence_b):\n",
    "    embeddings = model.encode([sentence_a, sentence_b])\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0, 0]\n",
    "\n",
    "entailment_pair = (\"A woman is jogging in the park\", \"A person is exercising outdoors\")\n",
    "neutral_pair = (\"He is typing on his laptop.\", \"The library is a quiet place to work.\")\n",
    "contradiction_pair = (\"The phone is fully charged.\", \"The battery is completely drained.\")\n",
    "\n",
    "calculate_and_print_similarity(model, tokenizer, huggingface_model, *entailment_pair, device, \"Entailment\")\n",
    "calculate_and_print_similarity(model, tokenizer, huggingface_model, *neutral_pair, device, \"Neutral\")\n",
    "calculate_and_print_similarity(model, tokenizer, huggingface_model, *contradiction_pair, device, \"Contradiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to sbert_model.pth\n"
     ]
    }
   ],
   "source": [
    "# todo\n",
    "torch.save(model.state_dict(), \"sbert_model.pth\")\n",
    "torch.save(classifier_head.state_dict(), \"classifier_head.pth\")\n",
    "\n",
    "print(\"Model saved to sbert_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
